{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **M1 KIVA SML Model Loan Prediction**\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "* Libraries & Data Importing\n",
    "\n",
    "* Data Preparation\n",
    "\n",
    "* Filter Countries to Top 20\n",
    "\n",
    "* Gender Mapping\n",
    "\n",
    "* Calculating Funding Duration\n",
    "\n",
    "* SML Preparation\n",
    "\n",
    "* Exporting Files for Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description\n",
    "\n",
    "#### Problem Statement:\n",
    "\n",
    "The objective is to predict the loan amount that a borrower can receive based on various factors such as the borrower's country, sector, activity type, number of borrowers, funding duration, and repayment term. By understanding these relationships, the model can provide insights into how these factors influence the size of loans that borrowers are likely to be granted.\n",
    "\n",
    "#### Type of Model:\n",
    "The model uses an ensemble approach, combining both Random Forest and XGBoost models. This ensemble method takes advantage of the strengths of both algorithms, providing a robust and accurate prediction of loan amounts by reducing overfitting and improving model performance. The target variable, `loan_amount`, is continuous, making regression the appropriate method.\n",
    "\n",
    "#### Objective:\n",
    "This ensemble model aims to assist lending platforms and financial institutions in estimating the appropriate loan amount for borrowers under different conditions. The model's insights can help improve loan approval strategies, ensure fair and data-driven lending decisions, and effectively manage borrower expectations. By accurately predicting loan amounts, the model helps lenders optimize resources, minimize risk, and better serve their customers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all requried libraries from requirement file (-r flag) and running in silent mode using -q flag\n",
    "!pip install -q -r https://raw.githubusercontent.com/JAdamHub/M1-Exam-Submission/refs/heads/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd             # for data manipulation\n",
    "import numpy as np              # for mathematical operations\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from scipy.stats import zscore  # for standardizing data & removing outliers\n",
    "from vega_datasets import data  # for data visualization\n",
    "\n",
    "#additional libraries for importing data from github\n",
    "import requests                 # used for importing data\n",
    "import io                       # used for converting response content to a file-like object for pandas\n",
    "\n",
    "# library used for gender mapping - section 4\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of url-paths for datasets\n",
    "# online\n",
    "url1 = 'https://raw.githubusercontent.com/JAdamHub/M1-Exam-Submission/refs/heads/main/data/kiva_loans_part_0.csv'\n",
    "url2 = 'https://raw.githubusercontent.com/JAdamHub/M1-Exam-Submission/refs/heads/main/data/kiva_loans_part_1.csv'\n",
    "url3 = 'https://raw.githubusercontent.com/JAdamHub/M1-Exam-Submission/refs/heads/main/data/kiva_loans_part_2.csv'\n",
    "\n",
    "# loading the urls into requests to download data\n",
    "response1 = requests.get(url1)\n",
    "response2 = requests.get(url2)\n",
    "response3 = requests.get(url3)\n",
    "\n",
    "# convert the response content to a file-like object for pandas using io.StringIO\n",
    "# the advantage of StringIO is that it allows us to import the data without having to save it as a file for pandas\n",
    "data_part1 = pd.read_csv(io.StringIO(response1.text))\n",
    "data_part2 = pd.read_csv(io.StringIO(response2.text))\n",
    "data_part3 = pd.read_csv(io.StringIO(response3.text))\n",
    "\n",
    "# We can see, that the imported loan dataset consists of 3 parts. We will like to combine these to one dataset\n",
    "data = pd.concat([data_part1, data_part2, data_part3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dropped rows: 97078\n",
      "In percentage 14.46 % of the data was removed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "funded_amount         0\n",
       "loan_amount           0\n",
       "activity              0\n",
       "sector                0\n",
       "country               0\n",
       "region                0\n",
       "posted_time           0\n",
       "disbursed_time        0\n",
       "funded_time           0\n",
       "term_in_months        0\n",
       "lender_count          0\n",
       "borrower_genders      0\n",
       "repayment_interval    0\n",
       "date                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping unnecessary columns\n",
    "data = data.drop(['tags', 'use', 'currency', 'country_code', 'partner_id'], axis=1)\n",
    "\n",
    "# we want to remove the missing rows from the dataset\n",
    "# storing length of rows for comparing, >> before dropna.. <<\n",
    "data_rows = len(data)\n",
    "\n",
    "# dropping missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# storing the now cleaned dataset\n",
    "cleaned_rows = len(data)\n",
    "\n",
    "# check..\n",
    "drops = data_rows - cleaned_rows\n",
    "\n",
    "# print the dropped rows and percentage of data removed\n",
    "print(f\"Number of dropped rows: {drops}\")\n",
    "print(f'In percentage {(drops / data_rows) * 100:.2f} % of the data was removed')\n",
    "# check after cleaning\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter countries to only include top 20 based on total loan_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the total amount of loan_amount for each country?\n",
    "country_loans = data.groupby('country')['loan_amount'].sum()\n",
    "\n",
    "# we would like the top 20 countries\n",
    "country_top_20 = country_loans.sort_values(ascending=False).head(20)\n",
    "\n",
    "# using index we create a new variable with the top 20 countries\n",
    "data = data[data['country'].isin(country_top_20.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borrower_genders</th>\n",
       "      <th>male_borrowers</th>\n",
       "      <th>female_borrowers</th>\n",
       "      <th>borrowers_count</th>\n",
       "      <th>male_borrower_ratio</th>\n",
       "      <th>gender_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female, female</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female(s)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   borrower_genders  male_borrowers  female_borrowers  borrowers_count  \\\n",
       "0            female               0                 1                1   \n",
       "1    female, female               0                 2                2   \n",
       "3            female               0                 1                1   \n",
       "4            female               0                 1                1   \n",
       "7            female               0                 1                1   \n",
       "8            female               0                 1                1   \n",
       "9            female               0                 1                1   \n",
       "10           female               0                 1                1   \n",
       "11           female               0                 1                1   \n",
       "12           female               0                 1                1   \n",
       "\n",
       "    male_borrower_ratio gender_class  \n",
       "0                   0.0    female(s)  \n",
       "1                   0.0    female(s)  \n",
       "3                   0.0    female(s)  \n",
       "4                   0.0    female(s)  \n",
       "7                   0.0    female(s)  \n",
       "8                   0.0    female(s)  \n",
       "9                   0.0    female(s)  \n",
       "10                  0.0    female(s)  \n",
       "11                  0.0    female(s)  \n",
       "12                  0.0    female(s)  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Gender Mapping (creating groups of male, female, and mixed gender groups)\n",
    "# count the number of male and female borrowers by searching for occurrences of 'male' and 'female' in the 'borrower_genders' column\n",
    "data['male_borrowers'] = data['borrower_genders'].apply(lambda x: len(re.findall(r'\\bmale', x)))\n",
    "data['female_borrowers'] = data['borrower_genders'].apply(lambda x: len(re.findall(r'\\bfemale', x)))\n",
    "\n",
    "# calculate the total number of borrowers by adding male and female borrower counts\n",
    "data['borrowers_count'] = data['male_borrowers'] + data['female_borrowers']\n",
    "\n",
    "# handle cases where 'borrowers_count' is 0 by replacing it with 1 to avoid division by zero\n",
    "data['male_borrower_ratio'] = data['male_borrowers'] / data['borrowers_count'].replace(0, 1)\n",
    "\n",
    "# function to classify the gender group based on the ratio of male borrowers\n",
    "# if the ratio is 1, it means all borrowers are male, so return 'male(s)'\n",
    "# if the ratio is 0, all borrowers are female, so return 'female(s)'\n",
    "# otherwise, return 'mixed gender group' for groups with both male and female borrowers\n",
    "def classify_genders(ratio):\n",
    "    if ratio == 1:\n",
    "        return 'male(s)'\n",
    "    elif ratio == 0:\n",
    "        return 'female(s)'\n",
    "    else:\n",
    "        return 'mixed gender group'\n",
    "\n",
    "# apply the gender classification function to each row based on the 'male_borrower_ratio'\n",
    "data['gender_class'] = data['male_borrower_ratio'].apply(classify_genders)\n",
    "\n",
    "# print of the first 10 rows of new added columns to see the borrower genders, counts, ratios, and classifications\n",
    "data[['borrower_genders', 'male_borrowers', 'female_borrowers', 'borrowers_count', 'male_borrower_ratio', 'gender_class']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19416"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Removing outliers based on loan_amount\n",
    "# calculate z-scores for loan_amount (our outlier detection)\n",
    "z_scores = zscore(data['loan_amount'])\n",
    "\n",
    "# get boolean array indicating the presence of outliers\n",
    "# using 2 & -2 z_scores to get 95% of data within 2 standard deviations\n",
    "data['outlier_loan_amount'] = (z_scores > 2) | (z_scores < -2)\n",
    "\n",
    "\n",
    "#removing outliers\n",
    "data_clean = data[~data['outlier_loan_amount']]\n",
    "\n",
    "# check amount of outliers\n",
    "data['outlier_loan_amount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 435525 entries, 0 to 221186\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   id                   435525 non-null  int64  \n",
      " 1   funded_amount        435525 non-null  float64\n",
      " 2   loan_amount          435525 non-null  float64\n",
      " 3   activity             435525 non-null  object \n",
      " 4   sector               435525 non-null  object \n",
      " 5   country              435525 non-null  object \n",
      " 6   region               435525 non-null  object \n",
      " 7   posted_time          435525 non-null  object \n",
      " 8   disbursed_time       435525 non-null  object \n",
      " 9   funded_time          435525 non-null  object \n",
      " 10  term_in_months       435525 non-null  float64\n",
      " 11  lender_count         435525 non-null  int64  \n",
      " 12  borrower_genders     435525 non-null  object \n",
      " 13  repayment_interval   435525 non-null  object \n",
      " 14  date                 435525 non-null  object \n",
      " 15  male_borrowers       435525 non-null  int64  \n",
      " 16  female_borrowers     435525 non-null  int64  \n",
      " 17  borrowers_count      435525 non-null  int64  \n",
      " 18  male_borrower_ratio  435525 non-null  float64\n",
      " 19  gender_class         435525 non-null  object \n",
      " 20  outlier_loan_amount  435525 non-null  bool   \n",
      "dtypes: bool(1), float64(4), int64(5), object(11)\n",
      "memory usage: 70.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating funding duration (time between posted_time and funded_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z7/j9rhznl118zbt_41ygz3rgyc0000gn/T/ipykernel_89346/3370879942.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loans['posted_time'] = pd.to_datetime(loans['posted_time'])\n",
      "/var/folders/z7/j9rhznl118zbt_41ygz3rgyc0000gn/T/ipykernel_89346/3370879942.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loans['funded_time'] = pd.to_datetime(loans['funded_time'])\n",
      "/var/folders/z7/j9rhznl118zbt_41ygz3rgyc0000gn/T/ipykernel_89346/3370879942.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loans['funding_duration'] = loans['funded_time'] - loans['posted_time']\n",
      "/var/folders/z7/j9rhznl118zbt_41ygz3rgyc0000gn/T/ipykernel_89346/3370879942.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loans['funding_duration_days'] = (loans['funded_time'] - loans['posted_time']).dt.total_seconds() / (24 * 60 * 60)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posted_time</th>\n",
       "      <th>funded_time</th>\n",
       "      <th>funding_duration</th>\n",
       "      <th>funding_duration_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01 06:12:39+00:00</td>\n",
       "      <td>2014-01-02 10:06:32+00:00</td>\n",
       "      <td>1 days 03:53:53</td>\n",
       "      <td>1.162419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01 06:51:08+00:00</td>\n",
       "      <td>2014-01-02 09:17:23+00:00</td>\n",
       "      <td>1 days 02:26:15</td>\n",
       "      <td>1.101562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01 08:03:11+00:00</td>\n",
       "      <td>2014-01-01 13:00:00+00:00</td>\n",
       "      <td>0 days 04:56:49</td>\n",
       "      <td>0.206123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01 11:53:19+00:00</td>\n",
       "      <td>2014-01-01 19:18:51+00:00</td>\n",
       "      <td>0 days 07:25:32</td>\n",
       "      <td>0.309398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014-01-01 11:46:01+00:00</td>\n",
       "      <td>2014-01-10 18:18:44+00:00</td>\n",
       "      <td>9 days 06:32:43</td>\n",
       "      <td>9.272720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                posted_time               funded_time funding_duration  \\\n",
       "0 2014-01-01 06:12:39+00:00 2014-01-02 10:06:32+00:00  1 days 03:53:53   \n",
       "1 2014-01-01 06:51:08+00:00 2014-01-02 09:17:23+00:00  1 days 02:26:15   \n",
       "3 2014-01-01 08:03:11+00:00 2014-01-01 13:00:00+00:00  0 days 04:56:49   \n",
       "4 2014-01-01 11:53:19+00:00 2014-01-01 19:18:51+00:00  0 days 07:25:32   \n",
       "7 2014-01-01 11:46:01+00:00 2014-01-10 18:18:44+00:00  9 days 06:32:43   \n",
       "\n",
       "   funding_duration_days  \n",
       "0               1.162419  \n",
       "1               1.101562  \n",
       "3               0.206123  \n",
       "4               0.309398  \n",
       "7               9.272720  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans = data_clean\n",
    "# We would like to also include the time/duration between posted_time and funded_time - in other words: how long it takes to get a loan funded\n",
    "# convert to pd.datetime\n",
    "loans['posted_time'] = pd.to_datetime(loans['posted_time'])\n",
    "loans['funded_time'] = pd.to_datetime(loans['funded_time'])\n",
    "\n",
    "# calculate time between posted_time and funded_time\n",
    "loans['funding_duration'] = loans['funded_time'] - loans['posted_time']\n",
    "\n",
    "# the result in days instead\n",
    "loans['funding_duration_days'] = (loans['funded_time'] - loans['posted_time']).dt.total_seconds() / (24 * 60 * 60)\n",
    "\n",
    "# first rows to check\n",
    "loans[['posted_time', 'funded_time', 'funding_duration', 'funding_duration_days']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SML Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SML Libaries importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder     # scaling numerical and encoding categorical data\n",
    "from sklearn.compose import ColumnTransformer                       # applying transformations to specific columns\n",
    "from sklearn.model_selection import RandomizedSearchCV              # hyperparameter tuning using random search\n",
    "from sklearn.ensemble import VotingRegressor                        # combine multiple models using voting\n",
    "import matplotlib.pyplot as plt                                     # plotting library for data visualization\n",
    "import seaborn as sns                                               # advanced data visualization library\n",
    "from sklearn.inspection import permutation_importance               # assessing feature importance through permutations\n",
    "import matplotlib.pyplot as plt                                     # plotting library (imported twice, should remove one)\n",
    "\n",
    "# SML models, cross-validation, and Regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score   # performance metrics for regression models\n",
    "from sklearn.model_selection import train_test_split                 # splitting data into training and testing sets\n",
    "from sklearn.linear_model import LinearRegression                    # linear regression model\n",
    "from sklearn.ensemble import RandomForestRegressor                   # Random forest regression model\n",
    "from sklearn.model_selection import cross_val_score                  # Cross-validation scoring for models\n",
    "from xgboost import XGBRegressor                                     # gradient boosting regressor from XGBoost\n",
    "from imblearn.under_sampling import RandomUnderSampler                # undersampling technique for handling imbalanced datasets\n",
    "from sklearn.tree import DecisionTreeRegressor                       # Decision tree regressor model\n",
    "\n",
    "# Pipeline for models\n",
    "from sklearn.pipeline import Pipeline                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify numeric columns in the dataset\n",
    "# We're selecting only the numeric columns for correlation analysis to check how related they are to each other.\n",
    "numeric_columns = loans.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Step 2: Calculate the correlation matrix\n",
    "# The correlation matrix tells us how strongly different numeric features are related (positively or negatively).\n",
    "corr_matrix = loans[numeric_columns].corr()\n",
    "\n",
    "# Step 3: Plot the correlation matrix\n",
    "# We use a heatmap to visually display the correlations between features.\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation Matrix for Numeric Columns in Loans Dataset')\n",
    "\n",
    "# Step 4: Display the plot in a clean layout\n",
    "# We ensure the plot fits the figure neatly before displaying.\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter relevant columns for SML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering only the relevant columns for the model\n",
    "relevant_columns = [\n",
    "    'loan_amount',\n",
    "    'activity',\n",
    "    'sector',\n",
    "    'country',\n",
    "    'region',\n",
    "    'borrowers_count',\n",
    "    'gender_class',\n",
    "    'funding_duration_days',\n",
    "    'term_in_months',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df = loans[relevant_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imbalance for country\n",
    "Some countries account for more data than others, which could lead to imbalanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line counts how many loans come from each country and displays the result.\n",
    "# .value_counts() shows the frequency of loans per country, while .to_string() ensures the full list is printed without truncation.\n",
    "print(loans_df['country'].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check initial distribution of countries\n",
    "print(\"Country distribution:\")\n",
    "print(loans_df['country'].value_counts())\n",
    "\n",
    "# Initialize RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=9000)\n",
    "\n",
    "# Apply resampling using 'country' as a balancing feature\n",
    "X_resampled, _ = rus.fit_resample(loans_df, loans_df['country'])\n",
    "\n",
    "print(\"Country distribution after balancing:\")\n",
    "print(X_resampled['country'].value_counts())\n",
    "\n",
    "# updating dataframe\n",
    "loans_df = X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know have substansially less data, which means it can be used directly for training models. \n",
    "loans_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL) Uncomment here to create a smaller sample for testing (5000 instead of 25360)\n",
    "# loans_df = loans_df.sample(n=5000, random_state=9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the target and feature variables + splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'loan_amount' as the target variable we want to predict.\n",
    "y = loans_df['loan_amount']\n",
    "\n",
    "# Select features (columns) we will use to predict the loan amount.\n",
    "X = loans_df[['activity', 'sector', 'country', 'borrowers_count', 'funding_duration_days', 'region', 'gender_class', 'term_in_months']]\n",
    "\n",
    "# Prepare numeric features for scaling to ensure they're on the same scale.\n",
    "numeric_features = ['borrowers_count', 'funding_duration_days', 'term_in_months']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# Prepare categorical features for encoding (turning them into numbers).\n",
    "categorical_features = ['activity', 'sector', 'country', 'region', 'gender_class']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine both numeric and categorical preprocessing into one step.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Split the data into training and testing sets (70% train, 30% test).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Linear Regression\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Pipeline for Decision Tree Regression\n",
    "pipeline_tree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "# Pipeline for Random Forest Regression\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    " \n",
    "# Pipeline for XGBoost\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training without hyperparameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Train a Linear Regression model using the training data\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to predict the loan amount on the test data\n",
    "y_pred_lr = pipeline_lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model by calculating MSE, MAE, and R² scores\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "# Check the model's accuracy on the training and test data\n",
    "train_score_lr = pipeline_lr.score(X_train, y_train)\n",
    "test_score_lr = pipeline_lr.score(X_test, y_test)\n",
    "\n",
    "# Print out the performance of the Linear Regression model\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(f\"  MSE: {mse_lr:.4f}\")\n",
    "print(f\"  MAE: {mae_lr:.4f}\")\n",
    "print(f\"  R²: {r2_lr:.4f}\")\n",
    "print(f\"Train Score: {train_score_lr:.4f}\")\n",
    "print(f\"Test Score: {test_score_lr:.4f}\")\n",
    "\n",
    "############################################################\n",
    "# Train a Random Forest model using the same process\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the loan amount on the test data\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model using the same metrics\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Check accuracy on training and test sets\n",
    "train_score_rf = pipeline_rf.score(X_train, y_train)\n",
    "test_score_rf = pipeline_rf.score(X_test, y_test)\n",
    "\n",
    "# Print out the Random Forest model's performance\n",
    "print(\"\\nRandom Forest Regression Performance:\")\n",
    "print(f\"  MSE: {mse_rf:.4f}\")\n",
    "print(f\"  MAE: {mae_rf:.4f}\")\n",
    "print(f\"  R²: {r2_rf:.4f}\")\n",
    "print(f\"Train Score: {train_score_rf:.4f}\")\n",
    "print(f\"Test Score: {test_score_rf:.4f}\")\n",
    "\n",
    "############################################################\n",
    "# Train an XGBoost model using the training data\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the loan amount on the test data\n",
    "y_pred_rf = pipeline_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate XGBoost with the same performance metrics\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Check the XGBoost model's accuracy on the training and test data\n",
    "train_score_rf = pipeline_xgb.score(X_train, y_train)\n",
    "test_score_rf = pipeline_xgb.score(X_test, y_test)\n",
    "\n",
    "# Print out the XGBoost model's performance\n",
    "print(\"\\nXGBoost Performance:\")\n",
    "print(f\"  MSE: {mse_rf:.4f}\")\n",
    "print(f\"  MAE: {mae_rf:.4f}\")\n",
    "print(f\"  R²: {r2_rf:.4f}\")\n",
    "print(f\"Train Score: {train_score_rf:.4f}\")\n",
    "print(f\"Test Score: {test_score_rf:.4f}\")\n",
    "\n",
    "############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamter tuning using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Random Forest model to reduce overfitting by adjusting parameters\n",
    "# Here we define a range of hyperparameters (e.g., n_estimators, max_depth) to try different settings for our Random Forest\n",
    "\n",
    "param_distributions = {\n",
    "    'regressor__n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
    "    'regressor__max_depth': [10, 15, 20],  # Maximum depth of each tree\n",
    "    'regressor__min_samples_split': [5, 10, 15],  # Minimum samples required to split a node\n",
    "    'regressor__min_samples_leaf': [3, 5, 7, 10]  # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# We use RandomizedSearchCV to find the best combination of hyperparameters for our Random Forest model\n",
    "random_search = RandomizedSearchCV(estimator=pipeline_rf, param_distributions=param_distributions,\n",
    "                                   n_iter=15, cv=5, n_jobs=-1, scoring='r2', verbose=1, random_state=69)\n",
    "\n",
    "# Train the model with different combinations of hyperparameters on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print out the best set of hyperparameters and the corresponding cross-validation R² score\n",
    "print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation R^2 score: {random_search.best_score_}\")\n",
    "\n",
    "# Check how well the tuned model performs on both the training and test sets using R² score\n",
    "train_r2 = random_search.score(X_train, y_train)\n",
    "test_r2 = random_search.score(X_test, y_test)\n",
    "print(f\"Random Forest - Training R^2 score: {train_r2}\")\n",
    "print(f\"Random Forest - Test R^2 score: {test_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Random Forest Regression\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=150, \n",
    "                                        min_samples_split=5, \n",
    "                                        min_samples_leaf=3,\n",
    "                                        max_depth=20))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation (Random Forest + XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST MODEL\n",
    "\n",
    "# Perform 5-fold cross-validation to assess model performance on different subsets of training data\n",
    "cv_scores = cross_val_score(pipeline_rf, X_train, y_train, cv=5, scoring='r2')\n",
    "\n",
    "# Train the model using the full training dataset\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate how well the model performs on the training and test datasets using R² score\n",
    "train_score_rf = pipeline_rf.score(X_train, y_train)\n",
    "test_score_rf = pipeline_rf.score(X_test, y_test)\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) and root mean squared error (RMSE) to evaluate prediction accuracy\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "# Output cross-validation scores, training/test R², and MSE/RMSE values\n",
    "print(f\"Random Forest\")\n",
    "print(f\"Cross-Validation R^2 Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation R^2: {cv_scores.mean()}\")\n",
    "print(f'Random Forest - Training R^2 score: {train_score_rf}')\n",
    "print(f'Random Forest - Test R^2 score: {test_score_rf}')\n",
    "print(f'Random Forest - Test MSE: {mse_rf}')\n",
    "print(f'Random Forest - Test RMSE: {rmse_rf}')\n",
    "\n",
    "####################################################################################\n",
    "# XGBOOST MODEL\n",
    "\n",
    "# Perform 5-fold cross-validation to assess XGBoost model performance\n",
    "cv_scores = cross_val_score(pipeline_xgb, X_train, y_train, cv=5, scoring='r2')\n",
    "\n",
    "# Train the XGBoost model on the full training dataset\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training and test datasets using R² score\n",
    "train_score_rf = pipeline_xgb.score(X_train, y_train)\n",
    "test_score_rf = pipeline_xgb.score(X_test, y_test)\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred_rf = pipeline_xgb.predict(X_test)\n",
    "\n",
    "# Calculate MSE and RMSE for prediction accuracy\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "# Output cross-validation scores, training/test R², and MSE/RMSE values\n",
    "print(f\"XGBoost\")\n",
    "print(f\"Cross-Validation R^2 Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation R^2: {cv_scores.mean()}\")\n",
    "print(f'XGBoost - Training R^2 score: {train_score_rf}')\n",
    "print(f'XGBoost - Test R^2 score: {test_score_rf}')\n",
    "print(f'XGBoost - Test MSE: {mse_rf}')\n",
    "print(f'XGBoost - Test RMSE: {rmse_rf}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ENSEMBLE MODEL - VOTING REGRESSOR\n",
    "# We combine the Random Forest and XGBoost pipelines into one ensemble model.\n",
    "# In this case, XGBoost has more weight (70%) as it tends to perform better for this dataset.\n",
    "\n",
    "ensemble_model = VotingRegressor(\n",
    "    estimators=[('rf', pipeline_rf), ('xgb', pipeline_xgb)],\n",
    "    weights=[0.3, 0.7]  # 30% Random Forest, 70% XGBoost\n",
    ")\n",
    "\n",
    "# Fit the ensemble model on the training data\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model on training data and print the R² score\n",
    "ensemble_train_score = ensemble_model.score(X_train, y_train)\n",
    "print(f\"Ensemble Model Training R² Score: {ensemble_train_score:.4f}\")\n",
    "\n",
    "# Evaluate the ensemble model on test data to check its generalization\n",
    "ensemble_test_score = ensemble_model.score(X_test, y_test)\n",
    "print(f\"Ensemble Model Test R² Score: {ensemble_test_score:.4f}\")\n",
    "\n",
    "# Comparing performance with the individual models (Random Forest & XGBoost)\n",
    "\n",
    "# Random Forest: Train and evaluate\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "rf_train_score = pipeline_rf.score(X_train, y_train)\n",
    "rf_test_score = pipeline_rf.score(X_test, y_test)\n",
    "print(f\"Random Forest Pipeline Training R² Score: {rf_train_score:.4f}\")\n",
    "print(f\"Random Forest Pipeline Test R² Score: {rf_test_score:.4f}\")\n",
    "\n",
    "# XGBoost: Train and evaluate\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "xgb_train_score = pipeline_xgb.score(X_train, y_train)\n",
    "xgb_test_score = pipeline_xgb.score(X_test, y_test)\n",
    "print(f\"XGBoost Pipeline Training R² Score: {xgb_train_score:.4f}\")\n",
    "print(f\"XGBoost Pipeline Test R² Score: {xgb_test_score:.4f}\")\n",
    "\n",
    "# Making predictions on the test set using the ensemble model\n",
    "y_pred = ensemble_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulization of Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the ensemble model (VotingRegressor)\n",
    "# The model is trained using the training data.\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate Permutation Feature Importance\n",
    "# We use permutation importance to measure how each feature impacts the model's performance.\n",
    "# The model's accuracy is checked with the feature randomly shuffled.\n",
    "# n_repeats=10 means we shuffle the feature 10 times to get a more reliable estimate.\n",
    "result = permutation_importance(ensemble_model, X_test, y_test, n_repeats=10, random_state=69, n_jobs=-1)\n",
    "\n",
    "# Plotting the feature importance based on the results of permutation\n",
    "# Features are sorted in ascending order for better visualization.\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# Creating a horizontal bar plot to display the importance of each feature.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx], align=\"center\")\n",
    "plt.yticks(range(len(sorted_idx)), X_test.columns[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.title(\"Permutation Feature Importance for Voting Regressor\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line generates predictions for the test set based on the ensemble model we've built.\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "# We create a scatter plot to compare the actual loan amounts (y_test) with the predicted loan amounts (y_pred).\n",
    "# Adding transparency (alpha=0.3) and smaller markers (s=10) makes it easier to see dense data points.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3, s=10, color='blue', label='Predictions')  # marker size = (s=), transparency = (alpha=)\n",
    "\n",
    "# Plotting the ideal fit line (red dashed line)\n",
    "# This line represents a perfect prediction, where the actual and predicted loan amounts are identical.\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red', label='Ideal Fit')\n",
    "\n",
    "# Adding plot title and axis labels for clarity\n",
    "plt.title('Ensemble Model: Actual vs Predicted Loan Amounts', fontsize=16)\n",
    "plt.xlabel('Actual Loan Amount', fontsize=14)\n",
    "plt.ylabel('Predicted Loan Amount', fontsize=14)\n",
    "\n",
    "# Add a legend to explain the blue dots (predictions) and the red line (ideal fit)\n",
    "plt.legend()\n",
    "\n",
    "# Adding a grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORTING FILES FOR STREAMLIT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting ensemble model to joblib format for usage in Streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ensemble_model.joblib']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(ensemble_model, 'ensemble_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export target X_test (loan_amount) & y_test (features) to csv to be used in Streamlit deployment visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('data/X_test.csv', index=False)\n",
    "y_test.to_csv('data/y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
